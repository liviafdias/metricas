{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "66dbfb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import httpx\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS \n",
    "from spacy.lang.es.stop_words import STOP_WORDS as stopesp\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "import string \n",
    "import spacy\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "import joblib  # <<-- para salvar e carregar modelo\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9cb6b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================== VARIÁVEIS DE AMBIENTE ======================\n",
    "load_dotenv()\n",
    "USERNAME = os.getenv(\"INSTAGRAM_USERNAME\")\n",
    "PASSWORD = os.getenv(\"INSTAGRAM_PASSWORD\")\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "edfab3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== CONFIGURAÇÃO SELENIUM ======================\n",
    "service = Service(r'chromedriver-win64\\\\chromedriver-win64\\\\chromedriver.exe')\n",
    "driver = webdriver.Chrome(service=service)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7e936e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== OPENAI EMBEDDINGS ======================\n",
    "embe = OpenAIEmbeddings(openai_api_key=API_KEY, model=\"text-embedding-3-large\")\n",
    "\n",
    "def get_embedding(text):\n",
    "    try:\n",
    "        return embe.embed_query(text)\n",
    "    except:\n",
    "        return np.nan\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f93d1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ====================== COLETA DE LINKS ======================\n",
    "def coletar_links(perfil):\n",
    "    driver.get(f\"https://www.instagram.com/{perfil}/\")\n",
    "    time.sleep(5)\n",
    "\n",
    "    post_links = set()\n",
    "    previous_length = 0\n",
    "\n",
    "    for _ in range(300):\n",
    "        links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "        for link in links:\n",
    "            href = link.get_attribute('href')\n",
    "            if href and ('/p/' in href or '/reel/' in href):\n",
    "                post_links.add(href)\n",
    "\n",
    "        if len(post_links) == previous_length:\n",
    "            break\n",
    "        previous_length = len(post_links)\n",
    "\n",
    "        driver.find_element(By.TAG_NAME, 'body').send_keys(Keys.END)\n",
    "        time.sleep(5)\n",
    "\n",
    "    return post_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c811f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== EXTRAÇÃO DE POSTS (ATUALIZADA) ======================\n",
    "def extrair_dados_post(url):\n",
    "    likes_pattern = r'(\\d+K|\\d+)\\s*likes'\n",
    "    comments_pattern = r'(\\d{1,3}(?:,\\d{3})*)\\s*comments'\n",
    "\n",
    "    texto, likes, comentarios, data_publicacao = np.nan, np.nan, np.nan, np.nan\n",
    "\n",
    "    try:\n",
    "        # Usando Selenium para abrir a postagem logado\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "\n",
    "        # ================= TEXTO =================\n",
    "        try:\n",
    "            meta_description = driver.find_element(By.XPATH, \"//meta[@property='og:title']\")\n",
    "            if meta_description:\n",
    "                texto = meta_description.get_attribute(\"content\").split(\":\")[-1].strip()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ================= DATA (ISO 8601) =================\n",
    "        try:\n",
    "            time_elem = driver.find_element(By.TAG_NAME, \"time\")\n",
    "            data_publicacao = time_elem.get_attribute(\"datetime\")  # exemplo: 2020-01-15T12:34:56.000Z\n",
    "            data_publicacao = pd.to_datetime(data_publicacao, utc=True).strftime(\"%Y-%m-%d\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # ================= LIKES e COMENTÁRIOS =================\n",
    "        try:\n",
    "            meta_desc = driver.find_element(By.XPATH, \"//meta[@name='description']\")\n",
    "            if meta_desc:\n",
    "                content_description = meta_desc.get_attribute(\"content\")\n",
    "\n",
    "                likes_match = re.search(likes_pattern, content_description)\n",
    "                likes = likes_match.group(1) if likes_match else np.nan\n",
    "\n",
    "                comments_match = re.search(comments_pattern, content_description)\n",
    "                comentarios = comments_match.group(1).replace(',', '') if comments_match else np.nan\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao acessar {url}: {e}\")\n",
    "\n",
    "    return texto, data_publicacao, likes, comentarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c89ab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== PROCESSAR PERFIL ======================\n",
    "def processar_perfil(marca, pais, perfil):\n",
    "    print(f\"1. Processando perfil: {perfil}...\")\n",
    "\n",
    "    novos_links = coletar_links(perfil)\n",
    "    print(f\"{len(novos_links)} links coletados para {perfil}.\")\n",
    "\n",
    "    novos_dados = []\n",
    "    for link in tqdm(novos_links, desc=f\"Processando {perfil}\"):\n",
    "        texto, data_publicacao, likes, comentarios = extrair_dados_post(link)\n",
    "        #time.sleep(random.uniform(4, 10))  # pausa entre cada post\n",
    "        embedding = get_embedding(texto)\n",
    "        novos_dados.append([marca, pais, perfil, link, data_publicacao, texto, embedding, likes, comentarios, datetime.now().strftime('%Y-%m-%d')])\n",
    "\n",
    "    df_novos = pd.DataFrame(novos_dados, columns=[\"MARCA\", \"PAIS\", \"PERFIL\", \"LINK_PUBLICACAO\", \"DT_PUBLICACAO\", \"TEXTO\", \"EMBEDDING\", \"N_CURTIDAS\", \"N_COMENTARIOS\", \"DT_DOWNLOAD\"])\n",
    "    \n",
    "    csv_path = os.path.join(\"output\", f\"{perfil}_processado.csv\")\n",
    "    df_novos.to_csv(csv_path, index=False, sep=\";\", encoding=\"utf-8\")\n",
    "    print(f\"Arquivo criado: {csv_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bbaf0e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================== ORGANIZAÇÃO DOS DIRETÓRIOS ======================\n",
    "def dividir_diretorios():\n",
    "    for file in os.listdir(\"output\"):\n",
    "        if file.endswith(\"_processado.csv\"):\n",
    "            df = pd.read_csv(os.path.join(\"output\", file), delimiter=\";\")\n",
    "            pais = df[\"PAIS\"].iloc[0]\n",
    "\n",
    "            if pais == \"BR\":\n",
    "                os.makedirs(\"output/br\", exist_ok=True)\n",
    "                os.rename(os.path.join(\"output\", file), os.path.join(\"output/br\", file))\n",
    "            elif pais == \"SE\":\n",
    "                os.makedirs(\"output/se\", exist_ok=True)\n",
    "                os.rename(os.path.join(\"output\", file), os.path.join(\"output/se\", file))\n",
    "\n",
    "    print(\"2. Arquivos divididos em diretórios 'br' e 'se'.\")\n",
    "\n",
    "def conversao_float(value):\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        if value.endswith('K'):\n",
    "            return float(value[:-1]) * 1000\n",
    "        elif value.endswith('M'):\n",
    "            return float(value[:-1]) * 1000000\n",
    "        else:\n",
    "            return float(value) \n",
    "    return float(value)\n",
    "\n",
    "def processar_dados():\n",
    "    for folder in [\"br\", \"se\"]:\n",
    "        folder_path = os.path.join(\"output\", folder)\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\"_processado.csv\"):\n",
    "                df = pd.read_csv(os.path.join(folder_path, file), delimiter=\";\")\n",
    "\n",
    "                df['N_COMENTARIOS'] = df['N_COMENTARIOS'].apply(conversao_float)\n",
    "                df['N_CURTIDAS'] = df['N_CURTIDAS'].apply(conversao_float)\n",
    "                df[\"N_COMENTARIOS\"] = df[\"N_COMENTARIOS\"].fillna(0.0)\n",
    "                df[\"N_CURTIDAS\"] = df[\"N_CURTIDAS\"].fillna(0.0)\n",
    "\n",
    "                df.to_csv(os.path.join(folder_path, file), index=False, sep=\";\", encoding=\"utf-8\")\n",
    "                print(f\"3. Arquivo processado: {file}\")\n",
    "\n",
    "def filtrar_dados():\n",
    "    for folder in [\"br\", \"se\"]:\n",
    "        folder_path = os.path.join(\"output\", folder)\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\"_processado.csv\"):\n",
    "                df = pd.read_csv(os.path.join(folder_path, file), delimiter=\";\")\n",
    "                df['DT_PUBLICACAO'] = pd.to_datetime(df['DT_PUBLICACAO'], errors='coerce')\n",
    "                inicio_periodo = pd.to_datetime('2010-01-01') # formato ISO (ano-mês-dia)\n",
    "                fim_periodo = pd.to_datetime('2025-01-31')\n",
    "                df_filtrado = df[(df['DT_PUBLICACAO'] >= inicio_periodo) & (df['DT_PUBLICACAO'] <= fim_periodo)]\n",
    "                \n",
    "                csv_filtrado_path = os.path.join(folder_path, f\"{file.split('_processado')[0]}_filtrado.csv\")\n",
    "                df_filtrado.to_csv(csv_filtrado_path, index=False, sep=\";\", encoding=\"utf-8\")\n",
    "                print(f\"4. Arquivo filtrado criado: {csv_filtrado_path}\")\n",
    "\n",
    "def juntar_arquivos_filtrados():\n",
    "    dfs = []\n",
    "    for folder in [\"br\", \"se\"]:\n",
    "        folder_path = os.path.join(\"output\", folder)\n",
    "        for file in os.listdir(folder_path):\n",
    "            if file.endswith(\"_filtrado.csv\"):\n",
    "                df = pd.read_csv(os.path.join(folder_path, file), delimiter=\";\")\n",
    "                dfs.append(df)\n",
    "\n",
    "    df_junto = pd.concat(dfs, ignore_index=True)\n",
    "    csv_junto_path = os.path.join(\"output\", \"todos_filtrados.csv\")\n",
    "    df_junto.to_csv(csv_junto_path, index=False, sep=\";\", encoding=\"utf-8\")\n",
    "    df_junto['DT_PUBLICACAO'] = pd.to_datetime(df_junto['DT_PUBLICACAO'], errors='coerce')\n",
    "    df_junto['DT_DOWNLOAD'] = pd.to_datetime(df_junto['DT_DOWNLOAD'], errors='coerce')\n",
    "    print(f\"5. Arquivo combinado criado: {csv_junto_path}\")\n",
    "    return df_junto\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7e09c7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:9: SyntaxWarning: invalid escape sequence '\\S'\n",
      "<>:9: SyntaxWarning: invalid escape sequence '\\S'\n",
      "C:\\Users\\Lívia\\AppData\\Local\\Temp\\ipykernel_2468\\129435290.py:9: SyntaxWarning: invalid escape sequence '\\S'\n",
      "  sentimentos = pd.read_csv(\"docs\\SENTIMENTO_EMB.csv\", sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "# ====================== TREINAR OU CARREGAR MODELO ======================\n",
    "def obter_modelo_sentimento():\n",
    "    modelo_path = \"modelo_sentimento.pkl\"\n",
    "    if os.path.exists(modelo_path):\n",
    "        print(\"Carregando modelo de sentimentos salvo...\")\n",
    "        clf = joblib.load(modelo_path)\n",
    "    else:\n",
    "        print(\"Treinando modelo de sentimentos...\")\n",
    "        sentimentos = pd.read_csv(\"docs\\SENTIMENTO_EMB.csv\", sep=\";\")\n",
    "        vetores = [eval(sentimentos.EMBEDDING.iloc[i]) for i in range(len(sentimentos))]\n",
    "        vetores = np.array(vetores)\n",
    "        y = sentimentos.sentimento.values\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(vetores, y, test_size=0.3, random_state=42)\n",
    "        clf = LogisticRegression(max_iter=10000, random_state=42).fit(X_train, y_train)\n",
    "\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(\"Acurácia:\", accuracy_score(y_test, y_pred))\n",
    "        print(\"F1-Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "        # Salvar modelo treinado\n",
    "        joblib.dump(clf, modelo_path)\n",
    "        print(f\"Modelo salvo em {modelo_path}\")\n",
    "\n",
    "    return clf\n",
    "\n",
    "def sentimento_predicao(x, clf):\n",
    "    try:\n",
    "        return clf.predict(np.array(eval(x)).reshape(1, -1))[0]\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def gerar_arquivos_sentimento(df, clf):\n",
    "    # Predição do sentimento\n",
    "    df['SENTIMENTO'] = df['EMBEDDING'].apply(lambda x: sentimento_predicao(x, clf))\n",
    "\n",
    "    # Extrair X e Y das embeddings\n",
    "    def extrair_xy(emb):\n",
    "        try:\n",
    "            arr = np.array(eval(emb))\n",
    "            return arr[0], arr[1]\n",
    "        except:\n",
    "            return np.nan, np.nan\n",
    "\n",
    "    df[['X', 'Y']] = df['EMBEDDING'].apply(lambda e: pd.Series(extrair_xy(e)))\n",
    "\n",
    "    # Renomear colunas conforme solicitado\n",
    "    df = df.rename(columns={\n",
    "        \"N_CURTIDAS\": \"CURTIDAS\",\n",
    "        \"N_COMENTARIOS\": \"COMENTARIOS\",\n",
    "        \"PAIS\": \"ORIGEM\"\n",
    "    })\n",
    "\n",
    "    # Reorganizar colunas na ordem correta\n",
    "    df_final = df[[\n",
    "        \"X\", \"Y\", \"MARCA\", \"DT_PUBLICACAO\", \"TEXTO\",\n",
    "        \"LINK_PUBLICACAO\", \"CURTIDAS\", \"COMENTARIOS\",\n",
    "        \"ORIGEM\", \"EMBEDDING\", \"SENTIMENTO\"\n",
    "    ]]\n",
    "\n",
    "    # Gerar arquivos finais separados por origem\n",
    "    df_br = df_final[df_final[\"ORIGEM\"] == \"BR\"].copy()\n",
    "    df_se = df_final[df_final[\"ORIGEM\"] == \"SE\"].copy()\n",
    "\n",
    "    df_br.to_csv(\"TSNE_BR_COM_SENTIMENTO.csv\", sep=\";\", index=False, encoding=\"utf-8\")\n",
    "    df_se.to_csv(\"TSNE_SE_COM_SENTIMENTO.csv\", sep=\";\", index=False, encoding=\"utf-8\")\n",
    "\n",
    "    print(\"6. Arquivos finais gerados: TSNE_BR_COM_SENTIMENTO.csv e TSNE_SE_COM_SENTIMENTO.csv\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ddd29051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4. Arquivo filtrado criado: output\\br\\bydautobrasil_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\br\\caoacherypremium_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\br\\citroenbelfort_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\br\\fiatbr_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\br\\fordbrasil_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\br\\nissanbrasil_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\br\\peugeotbelfort_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\br\\renaultbrasil_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\br\\toyotadobrasil_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\br\\volvocarbr_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\br\\vwbrasil_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\se\\byd.sverige_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\se\\fiatsverige_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\se\\fordsverige_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\se\\nissansverige_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\se\\renaultsverige_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\se\\toyotasverige_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\se\\volkswagensverige_filtrado.csv\n",
      "4. Arquivo filtrado criado: output\\se\\volvocarse_filtrado.csv\n",
      "5. Arquivo combinado criado: output\\todos_filtrados.csv\n",
      "Carregando modelo de sentimentos salvo...\n",
      "6. Arquivos finais gerados: TSNE_BR_COM_SENTIMENTO.csv e TSNE_SE_COM_SENTIMENTO.csv\n",
      "Processo completo concluído!\n"
     ]
    }
   ],
   "source": [
    "# ====================== EXECUÇÃO PRINCIPAL ======================\n",
    "try:\n",
    "    # driver.get(\"https://www.instagram.com/\")\n",
    "    # time.sleep(8)\n",
    "\n",
    "    # driver.find_element(By.NAME, \"username\").send_keys(USERNAME)\n",
    "    # driver.find_element(By.NAME, \"password\").send_keys(PASSWORD + Keys.RETURN)\n",
    "    # time.sleep(8)\n",
    "\n",
    "    # os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "    # arquivo_perfis = \"montadoras.csv\"\n",
    "    # df_perfis = pd.read_csv(arquivo_perfis, delimiter=\";\") if arquivo_perfis.endswith(\".csv\") else pd.read_excel(arquivo_perfis)\n",
    "\n",
    "    # for _, row in df_perfis.iterrows():\n",
    "    #     processar_perfil(row[\"MARCA\"], row[\"PAIS\"], row[\"PERFIL\"])\n",
    "\n",
    "    # processar_dados()\n",
    "    filtrar_dados()\n",
    "    df_junto = juntar_arquivos_filtrados()\n",
    "\n",
    "    clf = obter_modelo_sentimento()\n",
    "    gerar_arquivos_sentimento(df_junto, clf)\n",
    "\n",
    "finally:\n",
    "    # driver.quit()\n",
    "    print(\"Processo completo concluído!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
